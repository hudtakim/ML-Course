{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # this is required for handle the array\n",
    "from matplotlib import pyplot as plt # for visualization\n",
    "\n",
    "def plotData(x, y, pred): #a function to visualize the data\n",
    "    plt.plot(x, y, 'ro') # we plot x with y here\n",
    "    plt.plot(x, pred) # plotting x with  prediction value\n",
    "    plt.title(\"Linear Regression : Gradient Descent\") # just the title of the visualization\n",
    "    plt.xlabel(\"x\") # just a label\n",
    "    plt.ylabel(\"y1 and Pred\") #just a label\n",
    "    plt.legend([\"This is x,y\", \"This is x,Pred\"]) # to help reader know what data is it\n",
    "    plt.show() # show the visualization.\n",
    "    \n",
    "def gradient_descent(x,y):\n",
    "    m_curr = b_curr = 0\n",
    "    iterations = 100\n",
    "    n = len(x)\n",
    "    learning_rate = 0.01\n",
    "    cost_hist = list()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n",
    "        md = -(2/n)*sum(x*(y-y_predicted))\n",
    "        bd = -(2/n)*sum(y-y_predicted)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        cost_hist.append(cost)\n",
    "    \n",
    "    return cost_hist\n",
    "\n",
    "def linear_Regression(m, x, b):\n",
    "    ypred = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        ypred[i] = abs(m * x[i] + b)\n",
    "        print('Prediction:', ypred[i], 'Real:', y[i])\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[987.2222222222222,\n",
       " 447.19873635116585,\n",
       " 360.00299732598756,\n",
       " 345.41885099021147,\n",
       " 342.48437913936,\n",
       " 341.42518037293735,\n",
       " 340.6739869589319,\n",
       " 339.97942020781187,\n",
       " 339.301085175086,\n",
       " 338.6324160449132,\n",
       " 337.9722720929948,\n",
       " 337.32038352144593,\n",
       " 336.67662125467484,\n",
       " 336.0408798488739,\n",
       " 335.4130587050356,\n",
       " 334.7930590435943,\n",
       " 334.18078340636686,\n",
       " 333.57613556386906,\n",
       " 332.9790204877874,\n",
       " 332.38934433405535,\n",
       " 331.8070144277852,\n",
       " 331.23193924865285,\n",
       " 330.6640284165052,\n",
       " 330.1031926771551,\n",
       " 329.5493438883532,\n",
       " 329.0023950059357,\n",
       " 328.4622600701442,\n",
       " 327.92885419211586,\n",
       " 327.4020935405424,\n",
       " 326.8818953284945,\n",
       " 326.3681778004115,\n",
       " 325.86086021925155,\n",
       " 325.3598628538039,\n",
       " 324.8651069661572,\n",
       " 324.37651479932566,\n",
       " 323.8940095650278,\n",
       " 323.4175154316186,\n",
       " 322.9469575121721,\n",
       " 322.4822618527113,\n",
       " 322.02335542058523,\n",
       " 321.57016609299194,\n",
       " 321.1226226456426,\n",
       " 320.680654741568,\n",
       " 320.24419292006456,\n",
       " 319.8131685857772,\n",
       " 319.387513997919,\n",
       " 318.96716225962507,\n",
       " 318.5520473074385,\n",
       " 318.14210390092785,\n",
       " 317.7372676124339,\n",
       " 317.3374748169441,\n",
       " 316.94266268209276,\n",
       " 316.55276915828654,\n",
       " 316.16773296895235,\n",
       " 315.7874936009071,\n",
       " 315.4119912948476,\n",
       " 315.04116703595776,\n",
       " 314.67496254463475,\n",
       " 314.31332026732895,\n",
       " 313.956183367499,\n",
       " 313.603495716679,\n",
       " 313.25520188565747,\n",
       " 312.9112471357659,\n",
       " 312.5715774102758,\n",
       " 312.236139325903,\n",
       " 311.9048801644182,\n",
       " 311.5777478643609,\n",
       " 311.25469101285813,\n",
       " 310.9356588375437,\n",
       " 310.62060119857887,\n",
       " 310.30946858077243,\n",
       " 310.0022120857984,\n",
       " 309.69878342451153,\n",
       " 309.39913490935754,\n",
       " 309.103219446879,\n",
       " 308.81099053031335,\n",
       " 308.5224022322847,\n",
       " 308.2374091975848,\n",
       " 307.95596663604545,\n",
       " 307.6780303154991,\n",
       " 307.40355655482716,\n",
       " 307.13250221709495,\n",
       " 306.8648247027725,\n",
       " 306.600481943039,\n",
       " 306.3394323931716,\n",
       " 306.08163502601565,\n",
       " 305.82704932553753,\n",
       " 305.57563528045654,\n",
       " 305.3273533779563,\n",
       " 305.0821645974758,\n",
       " 304.8400304045759,\n",
       " 304.6009127448837,\n",
       " 304.3647740381118,\n",
       " 304.13157717215165,\n",
       " 303.90128549724136,\n",
       " 303.67386282020595,\n",
       " 303.44927339876836,\n",
       " 303.2274819359328,\n",
       " 303.0084535744372,\n",
       " 302.7921538912746]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4,6,8,10,13,15])\n",
    "y = np.array([1,2,2,3,7,13,15,18,90])\n",
    "\n",
    "c = gradient_descent(x,y)\n",
    "it = np.arange(len(c))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
